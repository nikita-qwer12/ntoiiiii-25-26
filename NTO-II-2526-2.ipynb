{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "!pip install -qU lightgbm --upgrade\n",
    "!pip install -qU optuna scikit-learn implicit sentence-transformers\n",
    "\n",
    "import importlib, sys, os, gc, time, math, random, datetime, warnings\n",
    "import numpy as np, pandas as pd, torch, lightgbm as lgb, optuna, sklearn\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\" –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. scikit-learn –≤–µ—Ä—Å–∏—è: {sklearn.__version__}\")\n",
    "print(f\"GPU: {'–µ—Å—Ç—å' if torch.cuda.is_available() else '–Ω–µ—Ç—É'}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "\n",
    "def check_lightgbm_gpu():\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        data = np.random.rand(50, 2)\n",
    "        label = np.random.randint(2, size=50)\n",
    "        train_data = lgb.Dataset(data, label=label)\n",
    "        params = {'objective': 'binary', 'device': 'gpu', 'verbose': -1}\n",
    "        booster = lgb.train(params, train_data, num_boost_round=1)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"GPU –¥–ª—è LightGBM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "lgb_gpu_available = check_lightgbm_gpu()\n",
    "print(f\"üöÄ LightGBM GPU support: {'‚úÖ' if lgb_gpu_available else '‚ùå'}\")\n",
    "\n",
    "if not hasattr(sklearn.base, '_fit_context'):\n",
    "    def _fit_context(*args, **kwargs):\n",
    "        return args[0]\n",
    "    sklearn.base.BaseEstimator._fit_context = _fit_context\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_all_seeds(seed=42):\n",
    "    \"\"\"–§–∏–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö —Å–ª—É—á–∞–π–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print(f\" –í—Å–µ —Å–∏–¥—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã: {seed}\")\n",
    "\n",
    "set_all_seeds(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\" GPU Type: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")\n",
    "print(f\" GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\" if torch.cuda.is_available() else \"N/A\")\n",
    "print(f\" Device: {device}\")\n",
    "\n",
    "data_dir = '/kaggle/input/qwerty/'                         #–£–°–¢–ê–ù–û–í–ò–¢–¨ –ó–î–ï–°–¨ –í–ê–® –ü–£–¢–¨\n",
    "for path in ['/kaggle/input/qwerty/','/kaggle/input/']:          #–£–°–¢–ê–ù–û–í–ò–¢–¨ –ó–î–ï–°–¨ –í–ê–® –ü–£–¢–¨\n",
    "    if os.path.exists(os.path.join(path, 'train.csv')):\n",
    "        data_dir = path\n",
    "        break\n",
    "\n",
    "print(f\" –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: {data_dir}\")\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "books_df = pd.read_csv(os.path.join(data_dir, 'books.csv'))\n",
    "users_df = pd.read_csv(os.path.join(data_dir, 'users.csv'))\n",
    "book_desc_df = pd.read_csv(os.path.join(data_dir, 'book_descriptions.csv'))\n",
    "\n",
    "print(f\" –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "print(f\"   Train: {train_df.shape}\")\n",
    "print(f\"   Test: {test_df.shape}\")\n",
    "print(f\"   Books: {books_df.shape}\")\n",
    "print(f\"   Users: {users_df.shape}\")\n",
    "print(f\"   Descriptions: {book_desc_df.shape}\")\n",
    "\n",
    "original_test_user_ids = test_df['user_id'].copy().values\n",
    "original_test_book_ids = test_df['book_id'].copy().values\n",
    "\n",
    "train_ratings = train_df[train_df['has_read'] == 1].copy()\n",
    "train_ratings['timestamp'] = pd.to_datetime(train_ratings['timestamp'])\n",
    "\n",
    "print(\" –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "\n",
    "train_merged = train_ratings.merge(books_df, on='book_id', how='left')\n",
    "train_merged = train_merged.merge(users_df, on='user_id', how='left')\n",
    "test_merged = test_df.merge(books_df, on='book_id', how='left')\n",
    "test_merged = test_merged.merge(users_df, on='user_id', how='left')\n",
    "\n",
    "train_merged = train_merged.merge(book_desc_df[['book_id', 'description']], on='book_id', how='left')\n",
    "test_merged = test_merged.merge(book_desc_df[['book_id', 'description']], on='book_id', how='left')\n",
    "\n",
    "print(f\" –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\")\n",
    "print(f\"   Train merged: {train_merged.shape}\")\n",
    "print(f\"   Test merged: {test_merged.shape}\")\n",
    "\n",
    "print(\" –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤(TF-IDF)...\")\n",
    "\n",
    "train_merged['description'] = train_merged['description'].fillna('')\n",
    "test_merged['description'] = test_merged['description'].fillna('')\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=800,  \n",
    "    ngram_range=(1, 2),  \n",
    "    min_df=3,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "tfidf_train = tfidf.fit_transform(train_merged['description'])\n",
    "tfidf_test = tfidf.transform(test_merged['description'])\n",
    "\n",
    "tfidf_cols = [f'tfidf_{i}' for i in range(tfidf_train.shape[1])]\n",
    "train_tfidf_df = pd.DataFrame(tfidf_train.toarray(), columns=tfidf_cols)\n",
    "test_tfidf_df = pd.DataFrame(tfidf_test.toarray(), columns=tfidf_cols)\n",
    "\n",
    "print(f\" TF-IDF —Å–æ–∑–¥–∞–Ω–æ: {len(tfidf_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
    "\n",
    "train_merged = pd.concat([\n",
    "    train_merged.reset_index(drop=True), \n",
    "    train_tfidf_df.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "test_merged = pd.concat([\n",
    "    test_merged.reset_index(drop=True),\n",
    "    test_tfidf_df.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "print(f\" –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(tfidf_cols)}\")\n",
    "print(f\" –†–∞–∑–º–µ—Ä train_merged –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {train_merged.shape}\")\n",
    "\n",
    "def create_advanced_features(df, train_df=None):\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['hour'] = df['timestamp'].dt.hour\n",
    "        df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "        df['month'] = df['timestamp'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        df['quarter'] = df['timestamp'].dt.quarter\n",
    "    \n",
    "    if train_df is not None:\n",
    "        book_popularity = train_df.groupby('book_id')['timestamp'].agg(\n",
    "            ['count', 'max', 'min']\n",
    "        ).rename(columns={'count': 'book_total_ratings', 'max': 'book_last_rating', 'min': 'book_first_rating'})\n",
    "        \n",
    "        df = df.merge(book_popularity, on='book_id', how='left')\n",
    "        \n",
    "        user_activity = train_df.groupby('user_id')['timestamp'].agg(\n",
    "            ['count', 'max', 'min']\n",
    "        ).rename(columns={'count': 'user_total_ratings', 'max': 'user_last_rating', 'min': 'user_first_rating'})\n",
    "        \n",
    "        df = df.merge(user_activity, on='user_id', how='left')\n",
    "        \n",
    "        if 'timestamp' in df.columns and 'user_last_rating' in df.columns:\n",
    "            df['days_since_user_last_rating'] = (df['timestamp'] - pd.to_datetime(df['user_last_rating'])).dt.days\n",
    "            df['days_since_user_last_rating'] = df['days_since_user_last_rating'].fillna(30)\n",
    "            df['days_since_user_first_rating'] = (df['timestamp'] - pd.to_datetime(df['user_first_rating'])).dt.days\n",
    "        \n",
    "        if 'timestamp' in df.columns and 'book_last_rating' in df.columns:\n",
    "            df['days_since_book_last_rating'] = (df['timestamp'] - pd.to_datetime(df['book_last_rating'])).dt.days\n",
    "            df['days_since_book_last_rating'] = df['days_since_book_last_rating'].fillna(30)\n",
    "            df['days_since_book_first_rating'] = (df['timestamp'] - pd.to_datetime(df['book_first_rating'])).dt.days\n",
    "    \n",
    "    if 'user_avg_rating' in df.columns and 'book_avg_rating' in df.columns:\n",
    "        df['user_book_rating_diff'] = df['user_avg_rating'] - df['book_avg_rating']\n",
    "        df['user_book_rating_ratio'] = df['user_avg_rating'] / (df['book_avg_rating'] + 1e-8)\n",
    "        df['user_book_rating_product'] = df['user_avg_rating'] * df['book_avg_rating']\n",
    "    \n",
    "    if 'user_rating_count' in df.columns and 'book_rating_count' in df.columns:\n",
    "        df['user_book_popularity_ratio'] = df['user_rating_count'] / (df['book_rating_count'] + 1e-8)\n",
    "        df['user_book_popularity_diff'] = df['user_rating_count'] - df['book_rating_count']\n",
    "    \n",
    "    if 'description' in df.columns:\n",
    "        df['description_length'] = df['description'].fillna('').apply(len)\n",
    "        df['has_description'] = (df['description'].fillna('') != '').astype(int)\n",
    "        df['description_word_count'] = df['description'].fillna('').str.split().str.len()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_aggregated_features(train_df, target_df):\n",
    "    user_stats = train_df.groupby('user_id')['rating'].agg(\n",
    "        ['mean', 'count', 'std', 'median', 'max', 'min']\n",
    "    ).rename(columns={\n",
    "        'mean': 'user_avg_rating',\n",
    "        'count': 'user_rating_count',\n",
    "        'std': 'user_rating_std',\n",
    "        'median': 'user_median_rating',\n",
    "        'max': 'user_max_rating',\n",
    "        'min': 'user_min_rating'\n",
    "    }).fillna(0).reset_index()\n",
    "    \n",
    "    book_stats = train_df.groupby('book_id')['rating'].agg(\n",
    "        ['mean', 'count', 'std', 'median', 'max', 'min']\n",
    "    ).rename(columns={\n",
    "        'mean': 'book_avg_rating',\n",
    "        'count': 'book_rating_count',\n",
    "        'std': 'book_rating_std',\n",
    "        'median': 'book_median_rating',\n",
    "        'max': 'book_max_rating',\n",
    "        'min': 'book_min_rating'\n",
    "    }).fillna(0).reset_index()\n",
    "    \n",
    "    author_stats = train_df.groupby('author_id')['rating'].agg(\n",
    "        ['mean', 'count', 'std', 'median']\n",
    "    ).rename(columns={\n",
    "        'mean': 'author_avg_rating',\n",
    "        'count': 'author_rating_count',\n",
    "        'std': 'author_rating_std',\n",
    "        'median': 'author_median_rating'\n",
    "    }).fillna(0).reset_index()\n",
    "    \n",
    "    target_df = target_df.merge(user_stats, on='user_id', how='left')\n",
    "    target_df = target_df.merge(book_stats, on='book_id', how='left')\n",
    "    target_df = target_df.merge(author_stats, on='author_id', how='left')\n",
    "    \n",
    "    for col in ['user_avg_rating', 'book_avg_rating', 'author_avg_rating',\n",
    "                'user_median_rating', 'book_median_rating', 'author_median_rating']:\n",
    "        target_df[col] = target_df[col].fillna(target_df[col].mean())\n",
    "    \n",
    "    for col in ['user_rating_count', 'book_rating_count', 'author_rating_count',\n",
    "                'user_rating_std', 'book_rating_std', 'author_rating_std']:\n",
    "        target_df[col] = target_df[col].fillna(0)\n",
    "    \n",
    "    return target_df\n",
    "\n",
    "train_merged = train_merged.sort_values('timestamp')\n",
    "split_date = train_merged.iloc[int(len(train_merged) * 0.8)]['timestamp']\n",
    "train_data = train_merged[train_merged['timestamp'] <= split_date].copy()\n",
    "valid_data = train_merged[train_merged['timestamp'] > split_date].copy()\n",
    "\n",
    "print(f\" –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—Ä–µ–º–µ–Ω–∏:\")\n",
    "print(f\"   Split date: {split_date}\")\n",
    "print(f\"   Train  {train_data.shape}\")\n",
    "print(f\"   Valid  {valid_data.shape}\")\n",
    "\n",
    "print(\" –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\")\n",
    "train_data = create_advanced_features(train_data)\n",
    "valid_data = create_advanced_features(valid_data, train_data)\n",
    "test_data = create_advanced_features(test_merged, train_merged)\n",
    "\n",
    "train_data = create_aggregated_features(train_data, train_data.copy())\n",
    "valid_data = create_aggregated_features(train_data, valid_data.copy())\n",
    "test_data = create_aggregated_features(train_merged, test_data.copy())\n",
    "\n",
    "print(\" –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\")\n",
    "\n",
    "base_features = [\n",
    "    'user_avg_rating', 'user_rating_count', 'user_rating_std', 'user_median_rating',\n",
    "    'user_max_rating', 'user_min_rating',\n",
    "    'book_avg_rating', 'book_rating_count', 'book_rating_std', 'book_median_rating',\n",
    "    'book_max_rating', 'book_min_rating',\n",
    "    'author_avg_rating', 'author_rating_count', 'author_rating_std', 'author_median_rating',\n",
    "    'publication_year', 'avg_rating', 'page_count',\n",
    "    'hour', 'day_of_week', 'month', 'is_weekend', 'quarter',\n",
    "    'book_total_ratings', 'user_total_ratings',\n",
    "    'days_since_user_last_rating', 'days_since_book_last_rating',\n",
    "    'days_since_user_first_rating', 'days_since_book_first_rating',\n",
    "    'user_book_rating_diff', 'user_book_rating_ratio', 'user_book_rating_product',\n",
    "    'user_book_popularity_ratio', 'user_book_popularity_diff',\n",
    "    'description_length', 'has_description', 'description_word_count'\n",
    "]\n",
    "\n",
    "tfidf_features = [col for col in train_data.columns if 'tfidf_' in col]\n",
    "\n",
    "all_features = base_features + tfidf_features\n",
    "\n",
    "final_features = [col for col in all_features if col in train_data.columns and col in test_data.columns]\n",
    "\n",
    "print(f\" –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(final_features)}):\")\n",
    "print(f\"    –ë–∞–∑–æ–≤—ã–µ: {len(base_features)}\")\n",
    "print(f\"    TF-IDF: {len(tfidf_features)}\")\n",
    "\n",
    "X_train = train_data[final_features].copy()\n",
    "y_train = train_data['rating']\n",
    "X_valid = valid_data[final_features].copy()\n",
    "y_valid = valid_data['rating']\n",
    "X_test = test_data[final_features].copy()\n",
    "\n",
    "for col in final_features:\n",
    "    if X_train[col].isna().any() or X_valid[col].isna().any() or X_test[col].isna().any():\n",
    "        mean_val = X_train[col].mean()\n",
    "        X_train[col] = X_train[col].fillna(mean_val)\n",
    "        X_valid[col] = X_valid[col].fillna(mean_val)\n",
    "        X_test[col] = X_test[col].fillna(mean_val)\n",
    "\n",
    "for col in final_features:\n",
    "    for df in [X_train, X_valid, X_test]:\n",
    "        if np.isinf(df[col]).any():\n",
    "            max_val = df[col][np.isfinite(df[col])].max()\n",
    "            min_val = df[col][np.isfinite(df[col])].min()\n",
    "            df[col] = df[col].replace([np.inf, -np.inf], [max_val, min_val])\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=final_features)\n",
    "X_valid = pd.DataFrame(scaler.transform(X_valid), columns=final_features)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=final_features)\n",
    "\n",
    "print(\" –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "\n",
    "def train_als_features(train_df, user_col='user_id', item_col='book_id', rating_col='rating', factors=64):\n",
    "    try:\n",
    "        from implicit.als import AlternatingLeastSquares\n",
    "        from scipy.sparse import csr_matrix\n",
    "        import scipy.sparse as sparse\n",
    "        \n",
    "        print(\" –û–±—É—á–µ–Ω–∏–µ ALS...\")\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "        train_df = train_df.copy()\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä—Å–∏–∏ implicit\n",
    "        try:\n",
    "            import implicit\n",
    "            print(f\" implicit –≤–µ—Ä—Å–∏—è: {implicit.__version__}\")\n",
    "        except:\n",
    "            print(\" –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤–µ—Ä—Å–∏—é implicit\")\n",
    "        \n",
    "        alpha = 40.0 \n",
    "        epsilon = 1e-8\n",
    "        \n",
    "        min_rating = train_df[rating_col].min()\n",
    "        max_rating = train_df[rating_col].max()\n",
    "        if max_rating > min_rating:\n",
    "            train_df['normalized_rating'] = (train_df[rating_col] - min_rating) / (max_rating - min_rating)\n",
    "        else:\n",
    "            train_df['normalized_rating'] = 0.5\n",
    "        \n",
    "        train_df['confidence'] = 1.0 + alpha * train_df['normalized_rating']\n",
    "        \n",
    "        all_users = pd.concat([train_df[user_col], valid_data[user_col]]).unique()\n",
    "        all_items = pd.concat([train_df[item_col], valid_data[item_col]]).unique()\n",
    "        \n",
    "        all_users = sorted(all_users)\n",
    "        all_items = sorted(all_items)\n",
    "        \n",
    "        user_to_idx = {user: idx for idx, user in enumerate(all_users)}\n",
    "        item_to_idx = {item: idx for idx, item in enumerate(all_items)}\n",
    "        \n",
    "        train_df['user_idx'] = train_df[user_col].map(user_to_idx).fillna(-1).astype(int)\n",
    "        train_df['item_idx'] = train_df[item_col].map(item_to_idx).fillna(-1).astype(int)\n",
    "        \n",
    "        valid_mask = (train_df['user_idx'] >= 0) & (train_df['item_idx'] >= 0)\n",
    "        train_df = train_df[valid_mask].copy()\n",
    "        \n",
    "        print(f\" –î–∞–Ω–Ω—ã–µ –¥–ª—è ALS: {len(train_df)} –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π, \"\n",
    "              f\"{len(all_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, {len(all_items)} –∫–Ω–∏–≥\")\n",
    "        \n",
    "        sparse_matrix = csr_matrix(\n",
    "            (train_df['confidence'].values, \n",
    "             (train_df['user_idx'].values, train_df['item_idx'].values)),\n",
    "            shape=(len(all_users), len(all_items)),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        use_gpu = False\n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                from implicit import gpu\n",
    "                has_gpu = gpu.HAS_CUDA\n",
    "                if has_gpu:\n",
    "                    use_gpu = True\n",
    "                    print(\" GPU –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è ALS\")\n",
    "                else:\n",
    "                    print(\" GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è ALS, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU\")\n",
    "            else:\n",
    "                print(\" CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è ALS\")\n",
    "        except Exception as e_gpu:\n",
    "            print(f\" –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ GPU –¥–ª—è ALS: {e_gpu}, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU\")\n",
    "    \n",
    "        model = AlternatingLeastSquares(\n",
    "            factors=factors,\n",
    "            regularization=0.01,  \n",
    "            iterations=20,       \n",
    "            use_gpu=use_gpu,\n",
    "            num_threads=4,       \n",
    "            random_state=42,      \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            model.fit(sparse_matrix.T, show_progress=False)  \n",
    "        except TypeError:\n",
    "       \n",
    "            model.fit(sparse_matrix.T)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\" ALS —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω –∑–∞ {training_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "        \n",
    "        user_embeddings = model.user_factors.toarray() if hasattr(model.user_factors, 'toarray') else model.user_factors\n",
    "        item_embeddings = model.item_factors.toarray() if hasattr(model.item_factors, 'toarray') else model.item_factors\n",
    "        \n",
    "        print(f\" –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏={user_embeddings.shape}, –∫–Ω–∏–≥–∏={item_embeddings.shape}\")\n",
    "        \n",
    "        if np.isnan(user_embeddings).any() or np.isnan(item_embeddings).any():\n",
    "            print(\" –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU –≤–µ—Ä—Å–∏—é\")\n",
    "            raise ValueError(\"NaN –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö\")\n",
    "        \n",
    "        return user_embeddings, item_embeddings, user_to_idx, item_to_idx\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" –û—à–∏–±–∫–∞ ALS: {str(e)}\")\n",
    "        \n",
    "        print(\" –°–æ–∑–¥–∞–Ω–∏–µ fallback –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏...\")\n",
    "        \n",
    "        book_popularity = train_df.groupby('book_id')['rating'].mean().to_dict()\n",
    "        user_activity = train_df.groupby('user_id')['rating'].count().to_dict()\n",
    "        \n",
    "        all_users = train_df[user_col].unique()\n",
    "        all_items = train_df[item_col].unique()\n",
    "        \n",
    "        user_embeddings = np.zeros((len(all_users), factors))\n",
    "        item_embeddings = np.zeros((len(all_items), factors))\n",
    "        \n",
    "        for i, user in enumerate(all_users):\n",
    "            if user in user_activity:\n",
    "                user_embeddings[i, 0] = user_activity[user]\n",
    "        \n",
    "        for i, item in enumerate(all_items):\n",
    "            if item in book_popularity:\n",
    "                item_embeddings[i, 0] = book_popularity[item]\n",
    "        \n",
    "        user_to_idx = {user: i for i, user in enumerate(all_users)}\n",
    "        item_to_idx = {item: i for i, item in enumerate(all_items)}\n",
    "        \n",
    "        print(\" –°–æ–∑–¥–∞–Ω—ã fallback –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏\")\n",
    "        return user_embeddings, item_embeddings, user_to_idx, item_to_idx\n",
    "\n",
    "user_embs, item_embs, user_to_idx, item_to_idx = train_als_features(\n",
    "    train_data[['user_id', 'book_id', 'rating']].copy(),\n",
    "    factors=64\n",
    ")\n",
    "\n",
    "if user_embs is not None and item_embs is not None:\n",
    "    train_data['user_idx'] = train_data['user_id'].map(user_to_idx).fillna(-1).astype(int)\n",
    "    train_data['item_idx'] = train_data['book_id'].map(item_to_idx).fillna(-1).astype(int)\n",
    "    \n",
    "    als_user_features = []\n",
    "    als_item_features = []\n",
    "    \n",
    "    for i in range(64):\n",
    "        train_data[f'als_user_{i}'] = train_data['user_idx'].apply(\n",
    "            lambda x: user_embs[x, i] if x >= 0 and x < user_embs.shape[0] else 0\n",
    "        )\n",
    "        train_data[f'als_item_{i}'] = train_data['item_idx'].apply(\n",
    "            lambda x: item_embs[x, i] if x >= 0 and x < item_embs.shape[0] else 0\n",
    "        )\n",
    "        als_user_features.append(f'als_user_{i}')\n",
    "        als_item_features.append(f'als_item_{i}')\n",
    "    \n",
    "    valid_data['user_idx'] = valid_data['user_id'].map(user_to_idx).fillna(-1).astype(int)\n",
    "    valid_data['item_idx'] = valid_data['book_id'].map(item_to_idx).fillna(-1).astype(int)\n",
    "    \n",
    "    for i in range(64):\n",
    "        valid_data[f'als_user_{i}'] = valid_data['user_idx'].apply(\n",
    "            lambda x: user_embs[x, i] if x >= 0 and x < user_embs.shape[0] else 0\n",
    "        )\n",
    "        valid_data[f'als_item_{i}'] = valid_data['item_idx'].apply(\n",
    "            lambda x: item_embs[x, i] if x >= 0 and x < item_embs.shape[0] else 0\n",
    "        )\n",
    "    \n",
    "    test_data['user_idx'] = test_data['user_id'].map(user_to_idx).fillna(-1).astype(int)\n",
    "    test_data['item_idx'] = test_data['book_id'].map(item_to_idx).fillna(-1).astype(int)\n",
    "    \n",
    "    for i in range(64):\n",
    "        test_data[f'als_user_{i}'] = test_data['user_idx'].apply(\n",
    "            lambda x: user_embs[x, i] if x >= 0 and x < user_embs.shape[0] else 0\n",
    "        )\n",
    "        test_data[f'als_item_{i}'] = test_data['item_idx'].apply(\n",
    "            lambda x: item_embs[x, i] if x >= 0 and x < item_embs.shape[0] else 0\n",
    "        )\n",
    "    \n",
    "    final_features.extend(als_user_features + als_item_features)\n",
    "    \n",
    "    print(f\" –î–æ–±–∞–≤–ª–µ–Ω–æ ALS –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(als_user_features) + len(als_item_features)}\")\n",
    "    \n",
    "    X_train = train_data[final_features].copy()\n",
    "    X_valid = valid_data[final_features].copy()\n",
    "    X_test = test_data[final_features].copy()\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=final_features)\n",
    "    X_valid = pd.DataFrame(scaler.transform(X_valid), columns=final_features)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=final_features)\n",
    "\n",
    "train_data_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data_lgb = lgb.Dataset(X_valid, label=y_valid, reference=train_data_lgb)\n",
    "\n",
    "print(f\" LightGBM –¥–∞—Ç–∞—Å–µ—Ç—ã –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:\")\n",
    "print(f\"   Train samples: {X_train.shape[0]}\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt']),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 50),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 0.95),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.8, 0.95),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 5, 10),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.0, 0.5),\n",
    "        'max_bin': 255,\n",
    "        'feature_pre_filter': False,\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'device': 'gpu' if lgb_gpu_available else 'cpu',\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data_lgb,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[valid_data_lgb],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "            lgb.log_evaluation(period=0)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_valid)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "    return rmse\n",
    "\n",
    "print(\" –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=200, show_progress_bar=True)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'device': 'gpu' if lgb_gpu_available else 'cpu',\n",
    "    'feature_pre_filter': False\n",
    "})\n",
    "\n",
    "print(f\" –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_params}\")\n",
    "print(f\" –õ—É—á—à–∏–π RMSE: {study.best_value:.4f}\")\n",
    "\n",
    "print(\" –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏...\")\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    train_data_lgb,\n",
    "    num_boost_round=5000,\n",
    "    valid_sets=[valid_data_lgb],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "        lgb.log_evaluation(period=50)\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_pred_valid = final_model.predict(X_valid)\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, y_pred_valid))\n",
    "print(f\" –§–∏–Ω–∞–ª—å–Ω—ã–π RMSE –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {rmse:.4f}\")\n",
    "\n",
    "test_preds = final_model.predict(X_test)\n",
    "test_preds = np.clip(test_preds, 0, 10)\n",
    "\n",
    "min_size = min(len(test_preds), len(original_test_user_ids), len(original_test_book_ids))\n",
    "submission_df = pd.DataFrame({\n",
    "    'user_id': original_test_user_ids[:min_size],\n",
    "    'book_id': original_test_book_ids[:min_size],\n",
    "    'rating_predict': test_preds[:min_size]\n",
    "})\n",
    "\n",
    "submission_df['rating_predict'] = submission_df['rating_predict'].fillna(5.0)\n",
    "\n",
    "output_dir = '/kaggle/working/'         #–£–°–¢–ê–ù–û–í–ò–¢–¨ –í–ê–® –ü–£–¢–¨ –ö–£–î–ê –ë–£–î–ï–¢ –°–û–ó–î–ê–ù –°–ê–ë–ú–ò–¢\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "submission_path = os.path.join(output_dir, 'submission2.csv')\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\" Submission —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {submission_path}\")\n",
    "print(f\" –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: min={submission_df['rating_predict'].min():.4f}, max={submission_df['rating_predict'].max():.4f}, mean={submission_df['rating_predict'].mean():.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n –í—Å–µ —ç—Ç–∞–ø—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω—ã!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
